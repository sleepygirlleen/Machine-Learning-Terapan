# -*- coding: utf-8 -*-
"""MT1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X-EQyA4G3BApBAP_3Q3KpPdgiJW6Dhle

#**Proyek Machine Learning Terapan 1**

- **Nama:** Sulistiani
- **Email:** lisasa2lilisa@gmail.com
- **ID Dicoding:** hi_itslizeu

## **Import Library**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
import numpy
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from IPython.display import display
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from typing import Dict, List, Tuple
from sklearn.model_selection import ShuffleSplit, cross_validate, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer
from sklearn.base import BaseEstimator
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

"""## **Import Dataset**"""

# Import dataset
df = pd.read_csv('student_habits_performance.csv')

"""## **Assessing Data**"""

# Menampilkan ukuran dataset
df.shape

# Menampilkan dataset
df.head()

# Informasi Dataset

df.info()

"""## **Checking Missing Values**"""

df.isnull().sum()

"""## **Cleaning Data**"""

# Hapus kolom yang tidak diperlukan

df.drop(columns=['student_id', 'social_media_hours', 'netflix_hours', 'part_time_job', 'parental_education_level', 'extracurricular_participation', 'internet_quality', 'diet_quality'], inplace=True)

df.head()

numeric_features = df.select_dtypes(include=np.number).columns.tolist()

for feature in numeric_features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[feature])
    plt.title(f'Box Plot of {feature}')
    plt.show()

Q1 = df[numeric_features].quantile(0.25)
Q3 = df[numeric_features].quantile(0.75)
IQR = Q3 - Q1

# Filter dataframe untuk hanya menyimpan baris yang tidak mengandung outliers pada kolom numerik
condition = ~((df[numeric_features] < (Q1 - 1.5 * IQR)) | (df[numeric_features] > (Q3 + 1.5 * IQR))).any(axis=1)
df_filtered_numeric = df.loc[condition, numeric_features]

# Menggabungkan kembali dengan kolom kategorikal
categorical_features = df.select_dtypes(include=['object']).columns
df = pd.concat([df_filtered_numeric, df.loc[condition, categorical_features]], axis=1)

"""### **Data Transformation**"""

df['study_bin'] = pd.cut(df['study_hours_per_day'],
                         bins=[-0.1, 0, 1, 3, 5, 8, 12],
                         labels=['0', '<=1', '1–3', '3–5', '5–8', '>8'])

"""**Explanation:**

0: Tidak belajar sama sekali

<=1: Belajar sangat singkat

1–3: Waktu belajar singkat - moderat

3–5: Umumnya cukup intensif

5–8: Sangat intensif

>8: Mungkin terlalu lama atau tidak realistis

# **Exploratory Data Analysis**
"""

mental_health = df.groupby('mental_health_rating')[['exam_score']].mean()
print(mental_health)

mental_health.plot(kind='bar', figsize=(8, 5))
plt.title('Distribusi Antar Mental Health dan Nilai Siswa')
plt.xlabel('Rating')
plt.ylabel('Nilai Siswa')
plt.xticks(rotation=0)
plt.show()

sleep = df.groupby('sleep_hours')[['exam_score']].mean()

sleep = sleep.reset_index()

sleep['sleep_bin'] = pd.cut(sleep['sleep_hours'],
                            bins=[0, 4.99, 6.99, 8.99, 24],
                            labels=['<5', '5–6', '7–8', '9+'])

grouped = sleep.groupby('sleep_bin')['exam_score'].mean().reset_index()

print(grouped)

plt.figure(figsize=(8, 5))
plt.bar(grouped['sleep_bin'], grouped['exam_score'], color='mediumseagreen')
plt.title('Rata-rata Nilai berdasarkan Kelompok Lama Tidur')
plt.xlabel('Kelompok Lama Tidur (jam)')
plt.ylabel('Rata-rata Nilai')
plt.show()

study = df.groupby('study_bin')[['exam_score']].mean()
print(study)

study.plot(kind='bar', figsize=(8, 5))
plt.title('Distribusi Antar Waktu Belajar dan Nilai Siswa')
plt.xlabel('Jam')
plt.ylabel('Nilai Siswa')
plt.xticks(rotation=0)
plt.show()

exercise = df.groupby('exercise_frequency')[['exam_score']].mean()
print(exercise)

exercise.plot(kind='bar', figsize=(8, 5))
plt.title('Distribusi Antar Mental Health dan Nilai Siswa')
plt.xlabel('Frekuensi')
plt.ylabel('Nilai Siswa')
plt.xticks(rotation=0)
plt.show()

"""## **Data Splitting**"""

# Variabel independen (fitur)
X = df[['mental_health_rating', 'sleep_hours', 'exercise_frequency', 'study_hours_per_day']]

# Variabel dependen
y = df[['exam_score']]

# Data splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""## **Modeling**

### **Random Forest**
"""

rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)

"""### **Decision Tree**"""

from sklearn.tree import DecisionTreeRegressor

# Inisialisasi dan latih model
dt = DecisionTreeRegressor(random_state=42)
dt.fit(X_train, y_train)

# Prediksi
y_pred_dt = dt.predict(X_test)

"""### **K-Nearest Neighbors**"""

from sklearn.linear_model import LinearRegression

# Inisialisasi dan latih model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Prediksi
y_pred_lr = lr.predict(X_test)

"""# **Evaluation**"""

# Fungsi evaluasi terstandarisasi
def evaluate_model(y_true, y_pred, model_name):
    print(f"\n=== Evaluasi {model_name} ===")
    print(f"MAE: {mean_absolute_error(y_true, y_pred):.4f}")
    print(f"MSE: {mean_squared_error(y_true, y_pred):.4f}")
    print(f"RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.4f}")
    print(f"R2 Score: {r2_score(y_true, y_pred):.4f}")

# Perbaikan warning shape (pastikan y berbentuk 1D array)
y_train = np.ravel(y_train)
y_test = np.ravel(y_test)

# ===== Evaluasi Random Forest =====
evaluate_model(y_test, y_pred, "Random Forest")

# ===== Evaluasi Decision Tree =====
evaluate_model(y_test, y_pred_dt, "Decision Tree")


# ===== Evaluasi Linear Regression =====
evaluate_model(y_test, y_pred_lr, "Linear Regression")

# Koefisien Regresi
coefficients = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': lr.coef_[0]
}).sort_values('Coefficient', ascending=False)
print("\nKoefisien Regresi:")
print(coefficients)

# ===== Visualisasi Komparasi Prediksi =====
plt.figure(figsize=(12,6))
plt.scatter(y_test, y_pred, alpha=0.5, label='Random Forest')
plt.scatter(y_test, y_pred_dt, alpha=0.5, label='Decision Tree')
plt.scatter(y_test, y_pred_lr, alpha=0.5, label='Linear Regression')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--')
plt.xlabel('Nilai Sebenarnya')
plt.ylabel('Prediksi')
plt.title('Perbandingan Prediksi Model')
plt.legend()
plt.show()

"""# **Hyperparameter Tuning**"""

from sklearn.model_selection import GridSearchCV

# 1. Membuat custom scoring untuk semua metrik
scoring = {
    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),
    'MSE': make_scorer(mean_squared_error, greater_is_better=False),
    'R2': make_scorer(r2_score),
    'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),
}

# 2. Fungsi untuk menampilkan hasil tuning
def print_tuning_results(grid_search):
    print(f"Best Parameters: {grid_search.best_params_}")
    cv_results = grid_search.cv_results_

    # Ambil hasil terbaik
    best_index = grid_search.best_index_
    metrics = {
        'MAE': -cv_results[f'mean_test_MAE'][best_index],
        'MSE': -cv_results[f'mean_test_MSE'][best_index],
        'RMSE': cv_results[f'mean_test_RMSE'][best_index],
        'R2': cv_results[f'mean_test_R2'][best_index]
    }

    # Tampilkan dalam tabel
    results_df = pd.DataFrame([metrics], index=['Best Model'])
    print("\nPerformance Metrics:")
    print(results_df.round(4))

    return metrics

# 3. Linear Regression Tuning
print("\n" + "="*50)
print(" LINEAR REGRESSION TUNING ")
print("="*50)
param_grid_lr = {
    'fit_intercept': [True, False],
    'positive': [True, False]
}
grid_lr = GridSearchCV(LinearRegression(), param_grid_lr, cv=5, scoring=scoring, refit='MSE')
grid_lr.fit(X_train, y_train)
lr_metrics = print_tuning_results(grid_lr)

# 4. Random Forest Tuning
print("\n" + "="*50)
print(" RANDOM FOREST TUNING ")
print("="*50)
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [10, 15, None],
    'min_samples_leaf': [1, 3]
}
grid_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf,
                      cv=5, scoring=scoring, refit='MSE', n_jobs=-1)
grid_rf.fit(X_train, y_train)
rf_metrics = print_tuning_results(grid_rf)

# 5. Decision Tree Tuning
print("\n" + "="*50)
print(" DECISION TREE TUNING ")
print("="*50)
param_grid_dt = {
    'max_depth': [5, 8, 10],
    'min_samples_split': [5, 10],
    'ccp_alpha': [0, 0.01]
}
grid_dt = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid_dt,
                     cv=5, scoring=scoring, refit='MSE')
grid_dt.fit(X_train, y_train)
dt_metrics = print_tuning_results(grid_dt)

# 6. Tabel Perbandingan Final
print("\n" + "="*50)
print(" FINAL MODEL COMPARISON ")
print("="*50)
comparison = pd.DataFrame([lr_metrics, rf_metrics, dt_metrics],
                         index=['Linear Regression', 'Random Forest', 'Decision Tree'])
print(comparison.round(4))